---
output: html_document
---

## Alcune ulteriori analisi, con R

Di seguito riportiamo alcune ulteriori analisi, basate su alcuni concetti del corso di _Analisi dei Dati Multidimensionali_, con uno sguardo più approfondito a livello statistico e meno adatto ad un pubblico generalista.
Per uniformità, consideriamo solo i partiti che hanno superato il 3% dei consensi nazionali e che erano candidati in tutte le circoscrizioni.

### Analisi delle correlazioni

Per prima cosa, vediamo quanto può dirci l'andamento di un partito sui risultati degli altri
```{r echo=FALSE, message=FALSE, warning=FALSE}
europee <- read.csv("VotiPerc_R.csv", header=TRUE)
X <- europee[,7:14]
names(X) <- c("FdI","PD","M5S", "FI", "Lega", "AVS", "SUE", "Azione")
library(GGally)
library(ggplot2)
ggpairs(X, 
        lower = list(continuous = "points"),   # Scatterplots
        upper = list(continuous = "cor"),     # Correlazioni
        diag = list(continuous = "density"))  # Densità
```

Vediamo come quasi tutte le correlazioni siano significative. Vista la grande dimensione e l'eterogeneità delle unità statistiche, anche correlazioni intorno allo 0.2 risultano interessanti. Notiamo, ad esempio, che un risultato migliore per Fratelli d'Italia porta a percentuali minori per tutti gli altri partiti tranne la Lega.
A questo proposito, si può osservare che, mentre Fratelli d'Italia e Lega sono positivamente correlati, la correlazione Forza Italia gli altri due partiti di centrodestra è negativa. Il diverso trend di Forza Italia è consistente con quanto visto in precedenza. Inolte, il Partito Democratico risulta positivamente correlato con il Movimento 5 Stelle e debolmente anche con Alleanza Verdi e Sinistra. Potrebbe risultare sorprendente, infine, la correlazione positiva tra Movimento 5 Stelle e Forza Italia.

### Analisi delle componenti principali

La [_Principal Component Analysis_](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) è un metodo di riduzione della dimensionalità con cui cerchiamo di spiegare la variabilità dei dati tramite un minor numero di variabili dette, appunto, componenti principali.

Utilizziamo i dati standardizzati per far emergere la relazione tra i partiti al netto delle loro percentuali medie.
```{r echo=FALSE, message=FALSE, warning=FALSE}
pc <- prcomp(X, scale=T, center=T)
plot(pc, type="l")
# eigvals <- (pc$sdev)^2
# cumsum(eigvals/sum(eigvals))
summary(pc)
```
In queste condizioni, vediamo che tre componenti principali da sole spiegano circa il 58% della varianza totale. Per i dati grezzi, invece, due componenti spiegano già il 91%. Ciò è dovuto al fatto che i partiti più piccoli hanno molta meno varianza nei risultati, in termini assoluti, rispetto ai partiti maggiori.

#### Biplot

```{r echo=FALSE, message=FALSE, warning=FALSE}
biplot(pc, xlabs=rep("°",dim(X)[1]))
```

I partiti le cui frecce indicano direzioni simili sono positivamente correlati, quelli a 90° sono incorrelati, mentre quelli che indicano direzioni opposte sono correlati negativamente. Il risultato è tuttavia un'approssimazione basata sulle prime due componenti principali, ovvero spiega solo il 43.5% della varianza totale.

### Analisi fattoriale

La [_Factor Analysis_](https://en.wikipedia.org/wiki/Factor_analysis) (FA) è una tecnica di riduzione della dimensionalità simile alla PCA. Tuttavia, si differenza nel fatto che interpreta le p (nel nostro caso 8) variabili osservate come risultato della realizzazione di m<<p varaibili non osservabili (dette, appunto, fattori).

```{r}
factanal(X, 4)
```

Ci si potrebbe perdere nell'interpretazione dei fattori, ma il test sulla bontà di questa riduzione di dimensionalità ha p-value indistinguibile da 0. Pertanto, l'analisi dei fattori non si presta bene a questo dataset.

